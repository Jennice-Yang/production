{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An initial training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jenni\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\dataframe\\__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv \n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "ft_dir = os.getenv(\"FEATURES_DATA\")\n",
    "ft_glob = glob(ft_dir+'/*.parquet')\n",
    "df = dd.read_parquet(ft_glob).compute().reset_index().dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_file = os.path.join(\n",
    "    os.getenv(\"PRICE_CSV_DATA\"), \n",
    "    'symbols_valid_meta.csv'\n",
    ")\n",
    "cat_df = (pd.read_csv(cat_file)\n",
    "          .rename(columns = {'Symbol': 'ticker'})[['ticker', 'Listing Exchange', 'Market Category']]\n",
    "          )\n",
    "df = df.merge(cat_df, on = 'ticker', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "+ Previously, we produced a features data set.\n",
    "+ Most times, one or more [preprocessing steps](https://scikit-learn.org/stable/modules/preprocessing.html#) steps will be applied to data.\n",
    "+ The most practical way to apply them is by arranging them in `Pipeline` objects, wchich are sequential transformations applied to data. \n",
    "+ It is convenient for us to label these transformations and there is a standard way of doing so.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "+ Transformations are classes that implement `fit` and `transform` methods.\n",
    "\n",
    "### StandardScaler\n",
    "\n",
    "+ For example, transform a numerical variable by standardizing it.\n",
    "- Standardization is removing the mean value of the feature and scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "$$\n",
    "z = \\frac{x-\\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "\n",
    "+  Using [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), one can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
       "       'source', 'Year', 'Close_lag_1', 'Listing Exchange', 'Market Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenni\\AppData\\Local\\Temp\\ipykernel_35100\\619560177.py:1: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = (df.assign(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>Year</th>\n",
       "      <th>Close_lag_1</th>\n",
       "      <th>Listing Exchange</th>\n",
       "      <th>Market Category</th>\n",
       "      <th>returns</th>\n",
       "      <th>positive_return</th>\n",
       "      <th>hi_lo</th>\n",
       "      <th>op_cl</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACM</td>\n",
       "      <td>2007-05-11</td>\n",
       "      <td>21.129999</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>21.129999</td>\n",
       "      <td>21.850000</td>\n",
       "      <td>21.850000</td>\n",
       "      <td>3692100.0</td>\n",
       "      <td>ACM.csv</td>\n",
       "      <td>2007</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>0.035545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>0.720001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACM</td>\n",
       "      <td>2007-05-14</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>22.650000</td>\n",
       "      <td>21.850000</td>\n",
       "      <td>22.299999</td>\n",
       "      <td>22.299999</td>\n",
       "      <td>1652800.0</td>\n",
       "      <td>ACM.csv</td>\n",
       "      <td>2007</td>\n",
       "      <td>21.850000</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>0.020595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.799999</td>\n",
       "      <td>0.299999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACM</td>\n",
       "      <td>2007-05-15</td>\n",
       "      <td>21.850000</td>\n",
       "      <td>22.420000</td>\n",
       "      <td>20.730000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2002500.0</td>\n",
       "      <td>ACM.csv</td>\n",
       "      <td>2007</td>\n",
       "      <td>22.299999</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>-0.058296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.690001</td>\n",
       "      <td>-0.850000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACM</td>\n",
       "      <td>2007-05-16</td>\n",
       "      <td>22.490000</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>22.940001</td>\n",
       "      <td>22.940001</td>\n",
       "      <td>5229300.0</td>\n",
       "      <td>ACM.csv</td>\n",
       "      <td>2007</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>0.092381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.450001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACM</td>\n",
       "      <td>2007-05-17</td>\n",
       "      <td>23.340000</td>\n",
       "      <td>24.049999</td>\n",
       "      <td>22.940001</td>\n",
       "      <td>23.010000</td>\n",
       "      <td>23.010000</td>\n",
       "      <td>2501800.0</td>\n",
       "      <td>ACM.csv</td>\n",
       "      <td>2007</td>\n",
       "      <td>22.940001</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>0.003051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.109999</td>\n",
       "      <td>-0.330000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361116</th>\n",
       "      <td>ZIXI</td>\n",
       "      <td>2003-06-25</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>4.180000</td>\n",
       "      <td>3.990000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>112100.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2003</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>-0.049412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>-0.110000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361117</th>\n",
       "      <td>ZIXI</td>\n",
       "      <td>2003-06-26</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>4.190000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>515300.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2003</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361118</th>\n",
       "      <td>ZIXI</td>\n",
       "      <td>2003-06-27</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>162400.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2003</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>-0.037500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361119</th>\n",
       "      <td>ZIXI</td>\n",
       "      <td>2003-06-30</td>\n",
       "      <td>3.840000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.720000</td>\n",
       "      <td>3.770000</td>\n",
       "      <td>3.770000</td>\n",
       "      <td>119900.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2003</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>-0.020779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-0.070000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361120</th>\n",
       "      <td>ZIXI</td>\n",
       "      <td>2003-07-01</td>\n",
       "      <td>3.720000</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>3.780000</td>\n",
       "      <td>3.780000</td>\n",
       "      <td>202100.0</td>\n",
       "      <td>ZIXI.csv</td>\n",
       "      <td>2003</td>\n",
       "      <td>3.770000</td>\n",
       "      <td>Q</td>\n",
       "      <td>Q</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>361033 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ticker       Date       Open       High        Low      Close  \\\n",
       "0         ACM 2007-05-11  21.129999  22.400000  21.129999  21.850000   \n",
       "1         ACM 2007-05-14  22.000000  22.650000  21.850000  22.299999   \n",
       "2         ACM 2007-05-15  21.850000  22.420000  20.730000  21.000000   \n",
       "3         ACM 2007-05-16  22.490000  23.400000  22.400000  22.940001   \n",
       "4         ACM 2007-05-17  23.340000  24.049999  22.940001  23.010000   \n",
       "...       ...        ...        ...        ...        ...        ...   \n",
       "361116   ZIXI 2003-06-25   4.150000   4.180000   3.990000   4.040000   \n",
       "361117   ZIXI 2003-06-26   4.040000   4.190000   3.860000   4.000000   \n",
       "361118   ZIXI 2003-06-27   4.000000   4.050000   3.790000   3.850000   \n",
       "361119   ZIXI 2003-06-30   3.840000   4.000000   3.720000   3.770000   \n",
       "361120   ZIXI 2003-07-01   3.720000   3.850000   3.650000   3.780000   \n",
       "\n",
       "        Adj Close     Volume    source  Year  Close_lag_1 Listing Exchange  \\\n",
       "0       21.850000  3692100.0   ACM.csv  2007    21.100000                N   \n",
       "1       22.299999  1652800.0   ACM.csv  2007    21.850000                N   \n",
       "2       21.000000  2002500.0   ACM.csv  2007    22.299999                N   \n",
       "3       22.940001  5229300.0   ACM.csv  2007    21.000000                N   \n",
       "4       23.010000  2501800.0   ACM.csv  2007    22.940001                N   \n",
       "...           ...        ...       ...   ...          ...              ...   \n",
       "361116   4.040000   112100.0  ZIXI.csv  2003     4.250000                Q   \n",
       "361117   4.000000   515300.0  ZIXI.csv  2003     4.040000                Q   \n",
       "361118   3.850000   162400.0  ZIXI.csv  2003     4.000000                Q   \n",
       "361119   3.770000   119900.0  ZIXI.csv  2003     3.850000                Q   \n",
       "361120   3.780000   202100.0  ZIXI.csv  2003     3.770000                Q   \n",
       "\n",
       "       Market Category   returns  positive_return     hi_lo     op_cl  target  \n",
       "0                       0.035545              1.0  1.270000  0.720001     1.0  \n",
       "1                       0.020595              1.0  0.799999  0.299999     0.0  \n",
       "2                      -0.058296              0.0  1.690001 -0.850000     1.0  \n",
       "3                       0.092381              1.0  1.000000  0.450001     1.0  \n",
       "4                       0.003051              1.0  1.109999 -0.330000     0.0  \n",
       "...                ...       ...              ...       ...       ...     ...  \n",
       "361116               Q -0.049412              0.0  0.190000 -0.110000     0.0  \n",
       "361117               Q -0.009901              0.0  0.330000 -0.040000     0.0  \n",
       "361118               Q -0.037500              0.0  0.260000 -0.150000     0.0  \n",
       "361119               Q -0.020779              0.0  0.280000 -0.070000     1.0  \n",
       "361120               Q  0.002653              1.0  0.200000  0.060000     0.0  \n",
       "\n",
       "[361033 rows x 18 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (df.assign(\n",
    "        returns = lambda x: x['Close']/x['Close_lag_1'] - 1, \n",
    "        positive_return = lambda x: 1.0*(x['returns'] > 0),\n",
    "        hi_lo = lambda x: x['High'] - x['Low'],\n",
    "        op_cl = lambda x: x['Close'] - x['Open']\n",
    "    ).groupby(['ticker'], group_keys=False).apply(\n",
    "        lambda x: x.assign(target = x['positive_return'].shift(-1))\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .dropna(subset = ['target'])\n",
    "    )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method StandardScaler.fit of StandardScaler()>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a StandardScaler object\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Fit the StandardScaler object with the returns data\n",
    "std_scaler.fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Transform the returns data using the fitted scaler\u001b[39;00m\n\u001b[0;32m      3\u001b[0m returns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreturns\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m scaled_returns_np \u001b[38;5;241m=\u001b[39m \u001b[43mstd_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m scaled_returns \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaled_returns_np, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m scaled_returns\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[1;32mc:\\Users\\jenni\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jenni\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1059\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform standardization by centering and scaling.\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \n\u001b[0;32m   1047\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03m        Transformed array.\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1059\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m     copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m   1062\u001b[0m     X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m   1063\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1064\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1071\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jenni\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\utils\\validation.py:1757\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Transform the returns data using the fitted scaler\n",
    "\n",
    "returns = df.returns.values.reshape(-1, 1)\n",
    "scaled_returns_np = std_scaler.transform(df.returns.values.reshape(-1, 1))\n",
    "scaled_returns = pd.DataFrame(scaled_returns_np, columns=['returns'])\n",
    "scaled_returns.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  OneHotEncoder\n",
    "\n",
    "+ Categorical features can be encoded as numerical values using `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Listing Exchange'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Use [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to encode a categorical variable as numerical.\n",
    "+ Important parameters:\n",
    "\n",
    "    - `categories` allows you to specify the categories to work with.\n",
    "    - `drop`: we can drop the `'first'` value (dummy encoding) or `'if_binary'`, a convenience setting for binary values.\n",
    "    - `handle_unknown` allows three options, `'error'`, `'ignore'`, and `'infrequent_if_exist'`, depending on what we want to do with new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder()\n",
    "onehot.fit(df[['Listing Exchange']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_enc = onehot.transform(df[['Listing Exchange']])\n",
    "listing_enc.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "+ It is impractical and costly to manipulate data \"by hand\". \n",
    "+ To manage data preprocessing steps within the cross-validation process use `Pipeline` objects.\n",
    "+ A [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) object allows us to sequentially apply transformation steps and, if required, a predictor.\n",
    "+ `Pipeline` objects compose transforms, i.e., classes that implement `transform` and `fit` methods.\n",
    "+ The purpose of `Pipeline` objects is to ensemble transforms and predictors to be used in cross-validation.\n",
    "+ A `Pipeline` is defined by a list of tuples.\n",
    "+ Each tuple is composed of `(\"name\", <ColumnTransformer>)`, the name of the step and the `<ColumnTransformer>` function of our chosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, cohen_kappa_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline(\n",
    "    [\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('knn', DecisionTreeClassifier(criterion = 'entropy', max_depth=3))\n",
    "\n",
    "    ]\n",
    ")\n",
    "pipe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = df[['Listing Exchange', 'Market Category']]\n",
    "Y0 = df['target']\n",
    "X0_train, X0_test, Y0_train, Y0_test = train_test_split(X0, Y0, test_size=0.2, random_state=42)\n",
    "\n",
    "pipe1.fit(X0_train, Y0_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = pipe1.predict(X0_train)\n",
    "Y_pred_test = pipe1.predict(X0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_poba_train = pipe1.predict_proba(X0_train)\n",
    "Y_proba_test = pipe1.predict_proba(X0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    'accuracy_score_train': accuracy_score(Y0_train, Y_pred_train),\n",
    "    'accuracy_score_test': accuracy_score(Y0_test, Y_pred_test),\n",
    "    'cohen_kappa_train': cohen_kappa_score(Y0_train, Y_pred_train),\n",
    "    'cohen_kappa_test': cohen_kappa_score(Y0_test, Y_pred_test),\n",
    "    'log_loss_train': log_loss(Y0_train, Y_poba_train),\n",
    "    'log_loss_test': log_loss(Y0_test, Y_proba_test),\n",
    "    'f1_score_train': f1_score(Y0_train, Y_pred_train),\n",
    "    'f1_score_test': f1_score(Y0_test, Y_pred_test)\n",
    "}\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The model does not show great performance, but the pipeline shows results. \n",
    "+ Below, we expand the pipeline to include more variables, and further we will work with more robust model selection pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ColumnTransformer\n",
    "\n",
    "+ Use [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) to apply transformers to specific columns of a DataFrame.\n",
    "+ In this case, we will scale numeric variables and apply one-hot encoding to categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric_transfomer', StandardScaler(), ['returns', 'Volume', 'op_cl', 'hi_lo'] ),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='infrequent_if_exist'), ['Listing Exchange', 'Market Category']), \n",
    "    ], remainder='drop'\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('preproc', transformer), \n",
    "        ('decisiontree', DecisionTreeClassifier(criterion = 'entropy', max_depth=3))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "The model selection process is an iterative process in which :\n",
    "\n",
    "+ Select schema and load data.\n",
    "+ Define a pipeline and its (hyper) parameters.\n",
    "\n",
    "    - Use ColumnTransformers to transform numeric and cateogrical variables.\n",
    "    - Hyperparameters can be defined independently of code. \n",
    "\n",
    "+ Implement a splitting strategy. \n",
    "\n",
    "    - Use [cross_validate]() to select several metrics and operational details.\n",
    "\n",
    "+ Measure performance.\n",
    "\n",
    "    - [Select metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "\n",
    "+ Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Testing Split\n",
    "\n",
    "+ The first spliting strategy is to use a training, validation, and test set.\n",
    "+ Training set will be used to fit the model.\n",
    "+ Validation set is used to evaluate hyperparameter choice.\n",
    "+ Testing set is used to evaluate performance on data the model has not yet seen.\n",
    "+ In this case we want to compare two models: \n",
    "\n",
    "    - Decision Tree with 3 minumum samples per leaf.\n",
    "    - Decision Tree with 10 minimum samples per leaf.\n",
    "\n",
    "![](./images/03b_train_validate_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters in pipeline steps\n",
    "\n",
    "+ One can obtain the parameters of a pipeline with `pipe.get_params()`.\n",
    "+ We can set any parameter of a pipeline with `pipe.set_parames(**kwargs)`. \n",
    "+ The input `**kwargs` is a dictionary of the params to be modified. Params of the steps are labeled with the name of the step followed by `__` and the name of the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ There are a few steps that we will repeat: \n",
    "\n",
    "    - Fit the candidate model on training data.\n",
    "    - Predict on training and test data.\n",
    "    - Compute training and test performance metrics.\n",
    "    - Return.\n",
    "\n",
    "+ We encapsulate this procedure in a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(clf, X_train, Y_train, X_test, Y_test):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred_train = clf.predict(X_train)\n",
    "    Y_pred_test = clf.predict(X_test)\n",
    "    Y_proba_train = clf.predict_proba(X_train)\n",
    "    Y_proba_test = clf.predict_proba(X_test)\n",
    "    performance_metrics = {\n",
    "        'log_loss_train': log_loss(Y_train, Y_proba_train),\n",
    "        'log_loss_test': log_loss(Y_test, Y_proba_test),\n",
    "        'cohen_kappa_train': cohen_kappa_score(Y_train, Y_pred_train),\n",
    "        'cohen_kappa_test': cohen_kappa_score(Y_test, Y_pred_test),\n",
    "        'f1_score_train': f1_score(Y_train, Y_pred_train),\n",
    "        'f1_score_test': f1_score(Y_test, Y_pred_test),\n",
    "        'accuracy_score_train': accuracy_score(Y_train, Y_pred_train),\n",
    "        'accuracy_score_test': accuracy_score(Y_test, Y_pred_test),\n",
    "    }\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema\n",
    "X = df[['returns', 'op_cl', 'hi_lo', 'Volume', 'Listing Exchange', 'Market Category']]\n",
    "Y = df['target']\n",
    "\n",
    "# Split the data\n",
    "X_rest, X_test, Y_rest, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "X_train, X_validate, Y_train,  Y_validate = train_test_split(X_rest, Y_rest, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hyperparameter configuration 2\n",
    "pipe_d3 = pipe.set_params(**{'decisiontree__max_depth': 3})\n",
    "res_d3 = evaluate_model(pipe_d3, X_train, Y_train, X_validate, Y_validate)\n",
    "res_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hyperparameter configuration 2\n",
    "pipe_d15 = pipe.set_params(**{'decisiontree__max_depth':15})\n",
    "res_d15 = evaluate_model(pipe_d15, X_train, Y_train, X_validate, Y_validate)\n",
    "res_d15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "+ Cross-validation is a resampling method.\n",
    "+ It is an iterative method applied to training data.\n",
    "+ Training data is divided into folds.\n",
    "+ Each fold is used once as a validation set and the rest of the folds are used for training.\n",
    "+ Test data is used for final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Scikit's Documentation ](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance), the diagram below shows the data divisions and folds during the cross-validation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/03b_grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two functions that can be used for [calculating cross-validation performance scores](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance): `cross_val_score()` and `cross_validate()`. The first function, [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score), is a convenience function to get quick perfromance calculations. We will discuss `cross_validate()` as it offers advantages over `cross_val_score()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining metrics\n",
    "\n",
    "+ Use [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) to measure one or more performance metrics and operational details.\n",
    "+ There are two advantages of using this function. From [Scikit's documentation](https://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation):\n",
    "\n",
    ">- It allows specifying multiple metrics for evaluation.\n",
    ">- It returns a dict containing fit-times, score-times (and optionally training scores, fitted estimators, train-test split indices) in addition to the test score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc', 'neg_log_loss', 'neg_brier_score']\n",
    "d3_dict = cross_validate(pipe_d3, X, Y, cv=5, scoring = scoring, return_train_score = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DataFrame form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(d3_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d15_dict = cross_validate(pipe_d15, X, Y, cv=5, scoring = scoring, return_train_score = True)\n",
    "pd.DataFrame(d15_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Performance\n",
    "\n",
    "+ Notice that in order to acquire information about our model and continue development, we are spending resources: time, electricity, equipment use, etc. As well, we are generating data and binary objects that implement our models (fitted `Pipeline` objects, for example).\n",
    "+ For certain applications, operating performance (latency or `'score_time'`) may be as important or more important than predictive performance metrics. \n",
    "+ Every experiment throws important information and we can log them, as well as run them systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(d15_dict).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
